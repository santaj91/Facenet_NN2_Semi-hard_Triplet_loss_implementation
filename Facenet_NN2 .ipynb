{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Facenet_NN2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        " !nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYimotkoQ0D5",
        "outputId": "2abecb96-b408-453f-c719-40ab070d1c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jul 24 07:39:28 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7-J7dw6QIcy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras import layers , Model , Sequential\n",
        "from tensorflow.keras.layers import Conv2D ,MaxPooling2D ,Flatten ,Dense, Dropout ,BatchNormalization,Input, AveragePooling2D \n",
        "import datetime\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "id": "cSFy5uW-c6RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=x_train[10000:20000]\n",
        "y_train = y_train[10000:20000]\n",
        "y_train=tf.cast(y_train,dtype=tf.float32)\n",
        "\n",
        "x_train = np.expand_dims(x_train,axis=-1)\n",
        "x_train = np.repeat(x_train, 3, axis=-1)/255.\n",
        "x_train=tf.image.resize(x_train, [224,224])\n",
        "x_train = tf.cast(x_train > 0.5, dtype=tf.float32)\n",
        "\n",
        "x_test=x_test[0:3000]\n",
        "y_test=y_test[0:3000]\n",
        "y_test=tf.cast(y_test,dtype=tf.float32)\n",
        "x_test = np.expand_dims(x_test,axis=-1)\n",
        "x_test = np.repeat(x_test, 3, axis=-1)/255.\n",
        "x_test=tf.image.resize(x_test, [224,224])\n",
        "x_test = tf.cast(x_test > 0.5, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "p2o_BZcj5-a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape,x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD9YgOR093l1",
        "outputId": "0ae32a9d-6989-40e2-c255-a37c3776beea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([10000, 224, 224, 3]), TensorShape([3000, 224, 224, 3]))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _pairwise_distances(embaddings):\n",
        "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2  , shape (batch_size, batch_size)\n",
        "    dot_product = tf.matmul(embaddings,tf.transpose(embaddings))# n n\n",
        "    square_norm = tf.linalg.diag_part(dot_product) # (n,)\n",
        "    distances_sqared = tf.expand_dims(square_norm, 0) + tf.expand_dims(square_norm, 1) - 2*dot_product# n n\n",
        "    distances= tf.sqrt(tf.math.maximum(distances_sqared,1e-16))# n n\n",
        "    distances = tf.cast(distances,dtype=tf.float32)\n",
        "\n",
        "    return distances"
      ],
      "metadata": {
        "id": "mRexhOoXbg0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_anchor_positive_triplet_mask(labels):\n",
        "    # Check if labels[i] == labels[j]\n",
        "    labels_equal= tf.cast(tf.expand_dims(labels,0)==tf.expand_dims(labels,1),dtype=tf.float32)\n",
        "    # Find index_i == index_j and than make them False\n",
        "    equal_indexes=tf.eye(tf.shape(labels)[0])\n",
        "    mask=labels_equal-equal_indexes\n",
        "    return mask"
      ],
      "metadata": {
        "id": "SQiQz0_-l8Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _nonequal_negative_triplet_mask(labels):\n",
        "    labels_equal= tf.cast(tf.expand_dims(labels,0)==tf.expand_dims(labels,1),dtype=tf.float32)\n",
        "    # Check if labels[i] != labels[j]\n",
        "    labels_nonequal = tf.ones((tf.shape(labels)[0],tf.shape(labels).shape[0]),dtype=tf.float32)-labels_equal\n",
        "    return labels_nonequal"
      ],
      "metadata": {
        "id": "lDPDZgt6q0Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_hard_triplet_loss(y_true,y_pred,margin,All_positive=False,semi=False):\n",
        "    labels= tf.squeeze(y_true,axis=-1)\n",
        "    \n",
        "    embeddings = y_pred\n",
        "\n",
        "    pairwise_dist=_pairwise_distances(embeddings)\n",
        "    positive_mask = _get_anchor_positive_triplet_mask(labels)\n",
        "    positive_distance = tf.multiply(pairwise_dist, positive_mask)\n",
        "    hardest_positive_distance = tf.reduce_max(positive_distance,axis=1,keepdims=True)\n",
        "    if All_positive == True:\n",
        "        loss = tf.reduce_mean(hardest_positive_distance)\n",
        "        return loss\n",
        "    # when taking maximum values away by a row in Matirx, minimum values are lower then zero. so we can choose minumum values by using function of minimum\n",
        "    nonequal_mask = _nonequal_negative_triplet_mask(labels)\n",
        "    distance_max = tf.math.reduce_max(pairwise_dist,1,keepdims=True)\n",
        "    hardest_negative_distance = tf.math.reduce_min(\n",
        "                    tf.cast(pairwise_dist-distance_max,dtype=tf.float32) * nonequal_mask,1,keepdims=True  \n",
        "                    ) + distance_max\n",
        "    if semi==False:\n",
        "        loss=trplet_loss= tf.maximum( hardest_positive_distance - hardest_negative_distance + margin, 0.)\n",
        "        loss= tf.reduce_mean(trplet_loss)\n",
        "        return loss\n",
        "    #semi hard\n",
        "    negative_selection_mask = tf.cast(hardest_positive_distance < hardest_negative_distance,dtype=tf.float32)\n",
        "\n",
        "    trplet_loss= tf.maximum(hardest_positive_distance - hardest_negative_distance + margin, 0.)*negative_selection_mask\n",
        "    loss= tf.reduce_mean(trplet_loss)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "9Taep5_qrMQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_function(margin=0.2,All_positive=False,semi=False):\n",
        "  def loss(y_true,y_pred):\n",
        "    return batch_hard_triplet_loss(y_true, y_pred,margin,All_positive,semi)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "0wgrkpQriThE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def l2_norm2d(x, pool_size = (3,3), strides = (1,1),\n",
        "             padding = 'same', data_format=None):\n",
        "    x = x ** 2\n",
        "    output = AveragePooling2D(pool_size, strides,padding)(x)# (a^2+b^2+c^2...)/n\n",
        "    output  = output*pool_size[0]*pool_size[1]# (a^2+b^2+c^2...)\n",
        "    output = tf.sqrt(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "0q-ATY13oTRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inception(input,input_size, x3_in , x3_out ,x5_in , x5_out , pooling_option='max' ,strides=(1,1),pool_strides=(1,1), pool_output=None ):\n",
        "   \n",
        "    x1= Conv2D(filters = input_size,strides=(1,1), kernel_size= (1,1), padding= 'same')(input)\n",
        "    x1=relu(x1)\n",
        "\n",
        "    x3=Conv2D(filters = x3_in,strides=(1,1), kernel_size= (1,1), padding= 'same')(input)\n",
        "    x3=Conv2D(filters = x3_out,strides=(strides) ,kernel_size= (3,3), padding= 'same')(x3)\n",
        "    x3=relu(x3)\n",
        "\n",
        "    x5=Conv2D(filters = x5_in,strides=(1,1), kernel_size= (1,1), padding= 'same')(input)\n",
        "    x5=Conv2D(filters = x5_out,strides=strides, kernel_size= (5,5), padding= 'same')(x5)\n",
        "    x5=relu(x5)\n",
        "\n",
        "    if pooling_option == 'max':\n",
        "        x_po = MaxPooling2D(pool_size=(3,3), strides = pool_strides, padding=\"same\")(input)\n",
        "        if pool_output == 0:\n",
        "            pass\n",
        "        else:\n",
        "            x_po = Conv2D(filters = pool_output, kernel_size= (1,1), padding= 'same')(x_po)\n",
        "    elif pooling_option == 'l2':\n",
        "        x_po = l2_norm2d(input)\n",
        "        x_po = Conv2D(filters = pool_output, kernel_size= (1,1), padding= 'same')(x_po)\n",
        "\n",
        "    if input_size==0:\n",
        "        result=tf.concat([x3,x5,x_po],axis=3)\n",
        "    else:\n",
        "        result =  tf.concat([x1,x3,x5,x_po],axis=3)\n",
        "    return result"
      ],
      "metadata": {
        "id": "LndfWciLX_V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# orginal\n",
        "input_shape= (224,224,3)\n",
        "relu=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)\n",
        "\n",
        "\n",
        "input=Input(shape=input_shape)\n",
        "# conv,max pool + norm\n",
        "x1= Conv2D(filters = 64 ,strides=(2,2), kernel_size= (7,7), padding= 'same')(input)\n",
        "x1=MaxPooling2D(pool_size=(3, 3), strides=(2,2), padding=\"same\")(x1)\n",
        "x1=BatchNormalization(momentum=0.99,epsilon=0.001)(x1)\n",
        "x1=relu(x1)\n",
        "# inception (2), norm + max pool\n",
        "x2= Conv2D(filters = 64, kernel_size= (1,1), padding= 'same')(x1)\n",
        "x2=Conv2D(filters = 192,strides=(1,1), kernel_size= (3,3), padding= 'same')(x2)\n",
        "x2=relu(x2)\n",
        "x2=BatchNormalization(momentum=0.99,epsilon=0.001)(x2)\n",
        "x2=MaxPooling2D(pool_size=(3, 3), strides=(2,2), padding=\"same\")(x2)\n",
        "#inception (3a) \n",
        "x3 = inception(input=x2,input_size=64,x3_in=96,x3_out=128,x5_in=16,x5_out=32,pooling_option='max',pool_output=32)\n",
        "#inception (3b) \n",
        "x4 = inception(input=x3,input_size=64,x3_in=96,x3_out=128,x5_in=32,x5_out=64,pooling_option='l2',pool_output=64)\n",
        "#inception (3c)\n",
        "x5 = inception(input=x4,input_size=0,x3_in=128,x3_out=256,x5_in=32,x5_out=64,pooling_option='max',pool_output=0, strides=(2,2),pool_strides=(2,2))\n",
        "#inception (4a)\n",
        "x6=inception(input=x5,input_size=256,x3_in=96,x3_out=192,x5_in=32,x5_out=64,pooling_option='l2',pool_output=128)\n",
        "#inception (4b)\n",
        "x7=inception(input=x6,input_size=224,x3_in=112,x3_out=224,x5_in=32,x5_out=64,pooling_option='l2',pool_output=128)\n",
        "# inception (4c)\n",
        "x8=inception(input=x7,input_size=192,x3_in=128,x3_out=256,x5_in=32,x5_out=64,pooling_option='l2',pool_output=128)\n",
        "#inception (4d) \n",
        "x9=inception(input=x8,input_size=160,x3_in=144,x3_out=288,x5_in=32,x5_out=64,pooling_option='l2',pool_output=128)\n",
        "#inception (4e) \n",
        "x10 = inception(input=x9,input_size=0,x3_in=160,x3_out=256,x5_in=64,x5_out=128,pooling_option='max',pool_output=0, strides=(2,2),pool_strides=(2,2))\n",
        "#inception (5a)\n",
        "x11 = inception(input=x10,input_size=384,x3_in=192,x3_out=384,x5_in=48,x5_out=128,pooling_option='l2',pool_output=128)\n",
        "#inception (5b)\n",
        "x12=inception(input=x11,input_size=384,x3_in=192,x3_out=384,x5_in=48,x5_out=128,pooling_option='max',pool_output=128)\n",
        "# avg pool\n",
        "x13= AveragePooling2D(pool_size=(7,7), strides=(1,1))(x12)\n",
        "x13= Flatten()(x13)\n",
        "#fully conn\n",
        "x14 = Dense(128)(x13)\n",
        "#L2 normalization\n",
        "x15=tf.math.l2_normalize(x14, axis=None, epsilon=1e-12, name='L2 normalization')\n",
        "\n",
        "NN2 = Model(input, [x15], name='nn2')\n",
        "\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "r0hpp_ehB3ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN2.load_weights('/content/drive/MyDrive/facenet/NN2_2.h5')"
      ],
      "metadata": {
        "id": "_E58IxrsxJL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('/content/drive/MyDrive/facenet/NN2.h5')"
      ],
      "metadata": {
        "id": "s2bgYGcQFjN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scheduler(epoch):\n",
        "    lr = 1e-6\n",
        "    if epoch <1100:\n",
        "        return lr/(1+0.01*epoch)\n",
        "    else:\n",
        "        return lr/(1+0.005*(epoch-500))\n",
        "\n",
        "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "EPOCH =2000\n",
        "filename='/content/drive/MyDrive/facenet/NN2_2.h5'\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(lr=0.005,epsilon=1e-7)\n",
        "# optimizer=tf.keras.optimizers.SGD(\n",
        "#     learning_rate=0.01, momentum=0.09\n",
        "# )\n",
        "\n",
        "NN2.compile(optimizer=optimizer,\n",
        "              loss=get_loss_function(margin=1,All_positive=False,semi=True)\n",
        "              )\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "checkpoint = ModelCheckpoint(filename,             \n",
        "                             verbose=1,            \n",
        "                             save_best_only=True,  \n",
        "                             save_weights_only=True\n",
        "                            \n",
        "                            )\n",
        "\n",
        "\n",
        "\n",
        "NN2.fit(x=x_train, \n",
        "          y=y_train, \n",
        "          epochs=EPOCH,\n",
        "          batch_size=BATCH_SIZE, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tensorboard_callback,callback,checkpoint])\n",
        "        # callbacks=[tensorboard_callback,callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_RSgiRbw3Ymc",
        "outputId": "98acd5d4-fef1-485d-d05c-03337d29d373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.6552\n",
            "Epoch 1: val_loss improved from inf to 0.59113, saving model to /content/drive/MyDrive/facenet/NN2_2.h5\n",
            "40/40 [==============================] - 36s 700ms/step - loss: 0.6552 - val_loss: 0.5911 - lr: 1.0000e-06\n",
            "Epoch 2/2000\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.6695\n",
            "Epoch 2: val_loss did not improve from 0.59113\n",
            "40/40 [==============================] - 24s 598ms/step - loss: 0.6695 - val_loss: 0.5917 - lr: 9.9010e-07\n",
            "Epoch 3/2000\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.6639\n",
            "Epoch 3: val_loss did not improve from 0.59113\n",
            "40/40 [==============================] - 24s 597ms/step - loss: 0.6639 - val_loss: 0.5931 - lr: 9.8039e-07\n",
            "Epoch 4/2000\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.6429\n",
            "Epoch 4: val_loss did not improve from 0.59113\n",
            "40/40 [==============================] - 24s 599ms/step - loss: 0.6429 - val_loss: 0.5927 - lr: 9.7087e-07\n",
            "Epoch 5/2000\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.6743\n",
            "Epoch 5: val_loss did not improve from 0.59113\n",
            "40/40 [==============================] - 24s 598ms/step - loss: 0.6743 - val_loss: 0.5943 - lr: 9.6154e-07\n",
            "Epoch 6/2000\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.6517\n",
            "Epoch 6: val_loss did not improve from 0.59113\n",
            "40/40 [==============================] - 24s 597ms/step - loss: 0.6517 - val_loss: 0.5949 - lr: 9.5238e-07\n",
            "Epoch 7/2000\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.6745\n",
            "Epoch 7: val_loss did not improve from 0.59113\n",
            "40/40 [==============================] - 24s 599ms/step - loss: 0.6745 - val_loss: 0.5962 - lr: 9.4340e-07\n",
            "Epoch 8/2000\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.6859\n",
            "Epoch 8: val_loss did not improve from 0.59113\n",
            "40/40 [==============================] - 24s 596ms/step - loss: 0.6859 - val_loss: 0.5978 - lr: 9.3458e-07\n",
            "Epoch 9/2000\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.6971\n",
            "Epoch 9: val_loss did not improve from 0.59113\n",
            "40/40 [==============================] - 24s 596ms/step - loss: 0.6971 - val_loss: 0.5984 - lr: 9.2593e-07\n",
            "Epoch 10/2000\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.6771\n",
            "Epoch 10: val_loss did not improve from 0.59113\n",
            "40/40 [==============================] - 24s 596ms/step - loss: 0.6771 - val_loss: 0.5997 - lr: 9.1743e-07\n",
            "Epoch 11/2000\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.6627\n",
            "Epoch 11: val_loss did not improve from 0.59113\n",
            "40/40 [==============================] - 24s 597ms/step - loss: 0.6627 - val_loss: 0.6007 - lr: 9.0909e-07\n",
            "Epoch 12/2000\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.6711\n",
            "Epoch 12: val_loss did not improve from 0.59113\n",
            "40/40 [==============================] - 24s 596ms/step - loss: 0.6711 - val_loss: 0.6063 - lr: 9.0090e-07\n",
            "Epoch 13/2000\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.6888\n",
            "Epoch 13: val_loss did not improve from 0.59113\n",
            "40/40 [==============================] - 24s 596ms/step - loss: 0.6888 - val_loss: 0.6063 - lr: 8.9286e-07\n",
            "Epoch 14/2000\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.6680\n",
            "Epoch 14: val_loss did not improve from 0.59113\n",
            "40/40 [==============================] - 24s 596ms/step - loss: 0.6680 - val_loss: 0.6066 - lr: 8.8496e-07\n",
            "Epoch 15/2000\n",
            "15/40 [==========>...................] - ETA: 13s - loss: 0.6405"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a27dcfa2b4f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m           callbacks=[tensorboard_callback,callback,checkpoint])\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;31m# callbacks=[tensorboard_callback,callback])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1387\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m     \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \"\"\"\n\u001b[1;32m   1222\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "# Evaluate the network\n",
        "results = NN2.predict(x_test)\n",
        "# Save test embeddings for visualization in projector\n",
        "np.savetxt(\"vecs.tsv\", results, delimiter='\\t')\n",
        "\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "[out_m.write(str(x) + \"\\n\") for x in np.array(y_test)]\n",
        "out_m.close()\n",
        "\n",
        "\n",
        "try:\n",
        "  from google.colab import files\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "iXCHiUVowkNO",
        "outputId": "c841e45a-14af-4d95-c366-88fc15ed7026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_19c6729a-8566-41db-8442-7521eee4c2cf\", \"vecs.tsv\", 9787945)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_682c9f7e-e1b0-4a5b-8e6e-103806ae0c1d\", \"meta.tsv\", 12000)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "    plt.imshow(x_test[i])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3YF-5tlJIoI0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}